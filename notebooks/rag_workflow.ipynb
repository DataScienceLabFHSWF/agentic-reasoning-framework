{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a4723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file downloaded\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "url = 'https://arxiv.org/pdf/2311.12983'\n",
    "filename = 'gaiapaper.pdf'\n",
    "# Use wget to download the file\n",
    "wget.download(url, out=filename)\n",
    "print('file downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84439f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "filename = 'gaiapaper.pdf'\n",
    "loader = PyPDFLoader(filename)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2164e7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01cffe87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GAIA:\\nA Benchmark for General AI Assistants\\nGr´ egoire Mialon1, Cl´ ementine Fourrier2, Craig Swift3, Thomas Wolf2, Yann LeCun1, Thomas\\nScialom4\\n1FAIR, Meta, 2HuggingFace, 3AutoGPT, 4GenAI, Meta\\nWe introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a\\nmilestone in AI research. GAIA proposes real-world questions that require a set of fundamental\\nabilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency.\\nGAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we\\nshow that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins. This notable\\nperformance disparity contrasts with the recent trend of LLMs outperforming humans on tasks\\nrequiring professional skills in e.g. law or chemistry. GAIA’s philosophy departs from the current\\ntrend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit\\nthat the advent of Artificial General '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content[:1000]  # Display the first 1000 characters of the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "877d6f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2229421/3068215441.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "/mnt/data3/rrao/projects/agentic-reasoning-framework/arf-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "model_path = \"./models/\"\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache_folder=model_path,\n",
    "    model_kwargs={'device': 'cpu'}  # or 'cuda' if you have GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96fdbb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data3/rrao/projects/agentic-reasoning-framework/arf-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document ingested\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "docsearch = Chroma.from_documents(texts, embedding_model)  # store the embedding in docsearch using Chromadb\n",
    "print('document ingested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4deca51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from tqdm import tqdm\n",
    "import time, re\n",
    "\n",
    "llm_main = ChatOllama(\n",
    "    model=\"llama3.1:latest\",\n",
    "    temperature=0.0,\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    api_key=\"ollama\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd845edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data3/rrao/projects/agentic-reasoning-framework/arf-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is GAIA in ten words?',\n",
       " 'result': 'GAIA is a benchmark for general AI assistant evaluation systems.',\n",
       " 'source_documents': [Document(metadata={'subject': '', 'total_pages': 24, 'source': 'gaiapaper.pdf', 'trapped': '/False', 'page': 17, 'creationdate': '2023-11-23T02:01:06+00:00', 'author': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'title': '', 'moddate': '2023-11-23T02:01:06+00:00', 'page_label': '18', 'keywords': ''}, page_content='C Extended description of GAIA\\nDescription of capabilities. When answering the questions, annotators specified the steps that were followed\\nand listed the tools they use. Based on the set of tools that were mentionned by the annotators, we defined\\ncapabilities required by GAIA. For each capability, we report examples of corresponding tool as reported by\\nannotators.\\n• Web browsing: tools related to search the web and browse websites. Examples: Web browser, Search\\nengine, Website widget access, Access to YouTube, Google Street View.\\n• Multi-modality: tools related to understanding data modality other than text. Examples: A speech-to-text\\ntool, Video recognition, Image recognition, OCR, Google Street View.\\n• Coding: tools related to code execution. Examples: Python, a calculator, Substitution cipher\\nencoder, C++ compiler, A word reversal tool / script.\\n• Diverse filetype reading : tools related to understanding various type of files given by a user or found\\non the web. Examples: PDF viewer, Excel file access, PowerPoint viewer, CSV access, Txt\\nfile access.\\n• N/A: tools for tasks that can currently be performed by non-augmented LLMs. Examples: Tetris\\nrules database, German translator, Spell checker, Text Editor, Bass note data.\\nNote that a tool can belong to different categories. For example, Google Street View requires access to the\\nweb, browsing, but also multi-modality. Hence, these categories are indications of the capabilities required by\\nGAIA and not a perfect typology of our questions.\\nFiletypes. Some GAIA questions come with additional files, whose distribution is given in Figure 6.\\n0 5 10 15 20 25 30\\nFile Count\\nxlsx\\npng\\npdf\\ntxt\\nmp3\\njpg\\ncsv\\ndocx\\npptx\\nzip\\nxml\\npy\\njson\\nm4a\\npdb\\nMOV\\njsonldFile Type\\n29\\n18\\n15\\n13\\n7\\n7\\n6\\n2\\n2\\n2\\n2\\n1\\n1\\n1\\n1\\n1\\n1\\nDistribution of File Types\\nFigure 6 Initial distributions of file types in GAIA.\\nDifficulty of the questions. Our analysis of the time taken by the annotators to answer a question shows a\\ncorrelation with the number of steps taken. The correlation is less clear with the number of different tools\\nused to answer.\\n18'),\n",
       "  Document(metadata={'keywords': '', 'source': 'gaiapaper.pdf', 'author': '', 'creationdate': '2023-11-23T02:01:06+00:00', 'trapped': '/False', 'page': 10, 'producer': 'pdfTeX-1.40.25', 'subject': '', 'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page_label': '11', 'creator': 'LaTeX with hyperref', 'moddate': '2023-11-23T02:01:06+00:00', 'total_pages': 24}, page_content='ensure the question admits only one correct answer and are therefore necessary. In practice, a user would ask\\nan under-specified question, and a useful assistant would answer by citing its sources or keeping the most\\ntrustworthy one. Both are difficult to factually evaluate, and we leave that aspect for future work.\\nLack of linguistic and cultural diversity. A big limitation of GAIA is its lack of language diversity: all\\nquestions are asked in “standard” English only, and many questions mostly rely on English web pages. This\\nbenchmark will therefore not validate the usefulness of assistants for non-English speakers (80% of the global\\nworld population), their usefulness on the non English-speaking web (about half of its content), nor on any\\nsort of dialectal variation of English. As such, GAIA is only a first step to estimate the potential of AI\\nassistants, but should not be seen as an absolute general proof of their success. We hope to fill this gap in\\nfuture work or through community involvement.\\n7 Acknowledgements\\nThe authors would like to thank Nicolas Usunier for suggesting the web search baseline, Edwin Chen for\\nhelping us improve our unusual protocol for annotators, Yacine Jernite for sharing his insights on diversity\\nwhen benchmark building, and Sasha Luccioni for taking the time to proofread some sections where proper\\nEnglish was eluding us.\\nReferences\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\\nMensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han,\\nZhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida\\nNematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and\\nKaren Simonyan. Flamingo: a visual language model for few-shot learning. In Alice H. Oh, Alekh Agarwal,\\nDanielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL\\nhttps://openreview.net/forum?id=EbMuimAbPbs.\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang,\\nKathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay,\\nKefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham,\\nJan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry,\\nChristopher A. Choquette-Choo, Aakanksha Chowdhery, Cl´ ement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa\\nDev, Jacob Devlin, Mark D´ ıaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus\\nFreitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou,\\nJoshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski,\\nWenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric\\nLi, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello\\nMaggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric\\nNi, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily\\nReif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby,\\nAmbrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran\\nVodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu,\\nLinting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\\nPetrov, and Yonghui Wu. Palm 2 technical report, 2023.\\nAnthropic. Model card and evaluations for claude models, 2023. URL https://www-files.anthropic.com/\\nproduction/images/Model-Card-Claude-2.pdf .\\nEmily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system\\nbias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587–604, 2018.\\nURL https://aclanthology.org/Q18-1041.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom\\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark\\n11'),\n",
       "  Document(metadata={'producer': 'pdfTeX-1.40.25', 'page_label': '3', 'total_pages': 24, 'creationdate': '2023-11-23T02:01:06+00:00', 'author': '', 'subject': '', 'title': '', 'moddate': '2023-11-23T02:01:06+00:00', 'keywords': '', 'trapped': '/False', 'page': 2, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with hyperref', 'source': 'gaiapaper.pdf'}, page_content='properties allow simple, fast and factual evaluation. Our questions are meant to be answered in zero\\nshot, limiting the influence of the evaluation setup. By opposition, many LLM benchmarks require\\nevaluations that are sensitive to the experimental setup such as the number and nature of prompts\\n(Liang et al., 2022b) (Section 8.2), or the benchmark implementation. 3\\nIn spite of being successful at tasks that are difficult for humans, the most capable LLMs do poorly on GAIA.\\nEven equipped with tools, GPT4 does not exceed a 30% success rate for the easiest of our tasks, and 0% for\\nthe hardest. In the meantime, the average success rate for human respondents is 92%. Consequently, a system\\ncapable of solving GAIA can be assessed in the context of t-AGI, 4 noting that humans typically take between\\n6 minutes for the simplest questions to 17 minutes for the most complex ones. From a related perspective,\\nsuch system would arguably be a competent General AI within the framework recently proposed in Morris\\net al. (2023), which also appear to be the next milestone in AI research since ChatGPT (OpenAI, 2023) is\\none level below. This paper covers the composition of GAIA, its design choices, and explain how to craft\\nquestions and the associated challenges so that the community can further extend the benchmark to target\\nemerging questions such as safety associated to tool use, or multi-modality. We also analyse the successes\\nand shortcomings of some of the most capable assistants to date, illustrating the potential of augmenting\\nLLMs. We release a developer set of 166 annotated questions and release the remaining 300 questions without\\nannotations: the benchmark will be notably hosted as a leaderboard. We hope our methodology will help\\naddressing the problem of open ended generation evaluation in NLP and beyond, and believe the successful\\nresolution of GAIA would be an important milestone towards the next generation of AI systems.\\n2 Related work\\nEvaluating Large Language Models. As LLMs capabilities have rapidly progressed, benchmarks become\\nsaturated at an increasing speed. As a example, reading comprehension was still a challenging task a few\\nyears alo (Rajpurkar et al., 2016). Wang et al. (2018) introduced the General Language Understanding\\nEvaluation benchmark (GLUE), on which models surpassed humans within a year. Its extension (Wang et al.,\\n2019) didn’t resist for more than a couple of years after its release. More generally, with each passing year,\\nstatic benchmarks are saturated and solved at human level at an ever increasing speed, as well illustrated by\\nKiela et al. (2023). While searching for harder evaluations, a natural direction is to explore tasks requiring\\nprofessional level knowledge in various fields such as law or science: an example is MMLU (Hendrycks et al.,\\n2021), containing over 15,000 questions covering 57 subjects across STEM, the humanities, the social sciences,\\nand more. And yet, LLMs already passed human performance on these, and have even been reported to\\nreach a stage where they could plausibly pass the US bar exam (OpenAI, 2023) or exceed the passing score\\non USMLE, a US examination program used to assess clinical competency and grant licensure (Nori et al.,\\n2023). Directions to evaluate LLMs more holistically, on their broader conversational aspects, have included\\n(i) compilations of evaluations (Gao et al., 2021; Liang et al., 2022a; Srivastava et al., 2023), which are\\noften difficult to aggregate meaningfully and are prone to contamination through data leakage, (ii) human\\nevaluation, which is time-consuming and difficult to scale, or (iii) model based evaluation to overcome this\\nlimitation (Zheng et al., 2023). However, this latter solution relies on using a more capable LLM (often GPT4)\\nthan the one currently evaluated, and the quality of the evaluation is affected by the shortcomings of the\\nevaluator LLM, which are not always obvious and can lead to subtly incorrect results.\\nEvaluating General Assistants. While there is ongoing effort to turn Large Language Models into general-\\npurpose assistants (see our discussion in Appendix A), appropriate evaluation is lagging behind. Most\\nevaluations rely on the use of closed systems, specific API calls, and a given “correct way” to attain the\\nanswer, or simply repurpose existing evaluation datasets. ToolQA (Zhuang et al., 2023) or Gentopia (Xu\\net al., 2023a) for example combine existing datasets with human annotations (MMLU, MATH, etc.) at the\\nrisk of contamination during training, and without ensuring tool usage is actually tested. Gorilla (Patil et al.,\\n2023) introduces APIBench, which tests how well an agent like system calls its specific API, similarly to\\nAPI-Bank (Li et al., 2023b), which provides an API pool to help the LLM during its evaluation. AgentBench\\n3https://huggingface.co/blog/evaluating-mmlu-leaderboard\\n4As defined in https://www.alignmentforum.org/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi , a t-AGI\\nbeats, on most tasks, most human experts who are given time t to perform the task\\n3'),\n",
       "  Document(metadata={'creator': 'LaTeX with hyperref', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creationdate': '2023-11-23T02:01:06+00:00', 'title': '', 'keywords': '', 'source': 'gaiapaper.pdf', 'moddate': '2023-11-23T02:01:06+00:00', 'author': '', 'producer': 'pdfTeX-1.40.25', 'trapped': '/False', 'page_label': '4', 'page': 3, 'subject': '', 'total_pages': 24}, page_content='(Liu et al., 2023a) is more general, and provides a number of closed box environments inside which assistant\\nLLMs can be deployed to answer user queries (from Unix shells to WebShopping APIs). However, because\\nsuch evaluations rely on closed environments, they risk evaluating how well the assistants have learned to\\nuse specific APIs, instead of more general results grounded in real world interactions. By opposition, GAIA\\ndoes not specify possible APIs, and relies on interactions with the real world. OpenAGI (Ge et al., 2023)\\nintroduces both a platform and a benchmark, made of a number of multi-steps tasks across modalities and\\ncapabilities, and is closer to our work. The core difference with GAIA is that their tasks focus on current\\nmodel capabilities rather than upcoming advancements.\\n3 GAIA\\nThis section covers the design and content of GAIA, as well as guidelines for creating questions and associated\\nchallenges.\\n3.1 A convenient yet challenging benchmark for general AI assistants\\nWhat is GAIA and how does it work? GAIA is a benchmark for AI systems proposing general assistant\\nquestions. GAIA attempts to circumvent different pitfalls of LLMs evaluation. It is composed of 466 questions\\ndesigned and annotated by humans. These questions are text-based, and sometimes come with a file (such as\\nan image or a spreadsheet). They cover various assistant use cases such as daily personal tasks, science, or\\ngeneral knowledge. The questions are designed to admit a short, single correct answer, therefore easy to verify.\\nTo use GAIA, one only needs to zero-shot prompt an AI assistant with the questions and attached evidence\\nif there are some. Scoring perfectly on GAIA requires a varied set of fundamental abilities (see Section 3.3).\\nWe provide questions along various with meta-data in supplementary material.\\nDesign choices. GAIA results both from the need for revised AI benchmarks, and the observed shortcomings\\nof LLM evaluation.\\nOur first principle is to target questions that are conceptually simple although potentially tedious for humans,\\nyet varied, rooted in the real world and challenging for current AI systems. This allows to focus on fundamental\\nabilities such as quick adaptation via reasoning, multi-modality understanding, and potentially diverse tool\\nuse, rather than specialised skills (Chollet, 2019). The questions generally consist in finding and transforming\\ninformation gathered from different and various sources, such as provided documents or the open and changing\\nweb, to produce an accurate answer. To answer the first example question above (Figure 1), LLMs should\\ntypically browse the web to find a study, then look for the correct enrolment. This goes against the trend\\nof benchmarks that are increasingly difficult for humans, and/or operate in purely textual or artificial\\nenvironments.\\nOur second principle is interpretability. The restricted number of highly curated questions makes the\\nbenchmark easier to use compared to aggregated ones (Perlitz et al., 2023). The conceptual simplicity of the\\ntask (human success rate is 92%) makes it easy for users to understand a model’s reasoning trace. For the\\nLevel 1 question from Figure 1, the reasoning trace will mostly consist in checking the correct website, and\\nreport the correct enrolment, which is simple to verify.\\nOur third principle is robustness against memorization: GAIA aims to be less gameable than most current\\nbenchmarks. To complete a task, a system has to plan and successfully complete some number of steps\\nsince the resulting answer is absent by design in plain text from current pre-training data. A progress in\\naccuracy reflects actual system progress. Due to their diversity and the size of the action space, these tasks\\ncannot be brute-forced without cheating, for example by memorizing the ground truth. Although accidental\\nmemorization is possible through data contamination, the accuracy required in the answers, their absence\\nfrom pre-training data, and the possibility to check the reasoning trace mitigate this risk. In contrast, multiple\\nchoice answers make contamination assessment difficult since a wrong reasoning trace can still get to the\\ncorrect choice. If catastrophic memorization happens in spite of these mitigations, it is easy to craft new\\nquestions using the guidelines we provide in Section 3.4.\\n4')]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_main, \n",
    "                                 chain_type=\"stuff\", \n",
    "                                 retriever=docsearch.as_retriever(), \n",
    "                                 return_source_documents=True)\n",
    "query = \"What is GAIA in ten words?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57e520b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data3/rrao/projects/agentic-reasoning-framework/arf-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Is Le Cunn one of the authors?',\n",
       " 'result': 'Yes, Yann LeCun is listed as an author in the paper \"Augmented language models: a survey\" by Grégoire Mialon et al., 2023.',\n",
       " 'source_documents': [Document(metadata={'creator': 'LaTeX with hyperref', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page_label': '15', 'trapped': '/False', 'author': '', 'subject': '', 'creationdate': '2023-11-23T02:01:06+00:00', 'keywords': '', 'total_pages': 24, 'moddate': '2023-11-23T02:01:06+00:00', 'producer': 'pdfTeX-1.40.25', 'page': 14, 'title': '', 'source': 'gaiapaper.pdf'}, page_content='Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene,\\nStefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven Piantadosi, Stuart Shieber,\\nSummer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali,\\nTatsunori Hashimoto, Te-Lin Wu, Th´ eo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius\\nNkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar\\nKhot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Venkatesh\\nRamasesh, vinay uday prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William\\nZhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh,\\nYair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov,\\nYu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi\\nWu. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions\\non Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj.\\nGeorge Stein, Jesse C. Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Leigh Ross, Valentin Villecroze, Zhaoyan Liu,\\nAnthony L. Caterini, J. Eric T. Taylor, and Gabriel Loaiza-Ganem. Exposing flaws of generative model evaluation\\nmetrics and their unfair treatment of diffusion models, 2023.\\nD´ ıdac Sur´ ıs, Sachit Menon, and Carl Vondrick. ViperGPT: Visual Inference via Python Execution for Reasoning,\\nMarch 2023.\\nYashar Talebirad and Amirhossein Nadiri. Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM\\nAgents, June 2023.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen,\\nGuillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj\\nGoswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez,\\nMadian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril,\\nJenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and\\nfine-tuned chat models, 2023.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task\\nbenchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop\\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, page 353–355. Association for Computational\\nLinguistics, Nov 2018. URL https://aclanthology.org/W18-5446.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel\\nBowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Advances\\nin Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual ChatGPT: Talking,\\nDrawing and Editing with Visual Foundation Models, March 2023.\\nBinfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, and\\nDongkuan Xu. Gentopia: A Collaborative Platform for Tool-Augmented LLMs, August 2023a.\\nQiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation capability\\nof open-source large language models, 2023b.\\nHui Yang, Sifu Yue, and Yunzhong He. Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions,\\nJune 2023.\\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari,\\nAveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic\\nModels: Composing Zero-Shot Multimodal Reasoning with Language, May 2022.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\\nDacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench\\nand chatbot arena, 2023.\\n15'),\n",
       "  Document(metadata={'creator': 'LaTeX with hyperref', 'moddate': '2023-11-23T02:01:06+00:00', 'trapped': '/False', 'title': '', 'page_label': '13', 'source': 'gaiapaper.pdf', 'author': '', 'creationdate': '2023-11-23T02:01:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'total_pages': 24, 'subject': '', 'keywords': '', 'page': 12, 'producer': 'pdfTeX-1.40.25'}, page_content='Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto,\\nThomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta\\nKoreeda. Holistic Evaluation of Language Models, November 2022a.\\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak\\nNarayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian\\nCosgrove, Christopher D. Manning, Christopher R´ e, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin\\nDurmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr,\\nLucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter\\nHenderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto,\\nThomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta\\nKoreeda. Holistic evaluation of language models, 2022b.\\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan\\nYang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su,\\nHuan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:\\n2308.03688, 2023a.\\nZhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen,\\nJuan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. BOLAA:\\nBenchmarking and Orchestrating LLM-augmented Autonomous Agents, August 2023b.\\nGr´ egoire Mialon, Roberto Dess` ı, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste\\nRozi` ere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom.\\nAugmented language models: a survey, 2023.\\nMicrosoft. Semantic Kernel. github, September 2023.\\nMeredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement\\nFarabet, and Shane Legg. Levels of agi: Operationalizing progress on the path to agi, 2023.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu\\nJain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback.\\narXiv preprint arXiv:2112.09332, 2021.\\nHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical\\nchallenge problems, 2023.\\nOpenAI. Gpt-4 technical report, 2023.\\nAnton Osika. GPT Engineer, September 2023.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\\nAmanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow\\ninstructions with human feedback, 2022.\\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large Language Model Connected with\\nMassive APIs, May 2023.\\nYotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer,\\nand Leshem Choshen. Efficient benchmarking (of language models), 2023.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine\\ncomprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,\\npages 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/\\nD16-1264. URL https://aclanthology.org/D16-1264.\\nSasha Rush. MiniChain, September 2023.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess` ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\\ncedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint\\narXiv:2302.04761, 2023.\\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving AI\\nTasks with ChatGPT and its Friends in Hugging Face, May 2023.\\n13'),\n",
       "  Document(metadata={'keywords': '', 'author': '', 'moddate': '2023-11-23T02:01:06+00:00', 'page_label': '16', 'creator': 'LaTeX with hyperref', 'total_pages': 24, 'subject': '', 'trapped': '/False', 'producer': 'pdfTeX-1.40.25', 'title': '', 'source': 'gaiapaper.pdf', 'creationdate': '2023-11-23T02:01:06+00:00', 'page': 15, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'}, page_content='Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. ToolQA: A Dataset for LLM Question Answering\\nwith External Tools, June 2023.\\n16'),\n",
       "  Document(metadata={'producer': 'pdfTeX-1.40.25', 'moddate': '2023-11-23T02:01:06+00:00', 'title': '', 'keywords': '', 'creationdate': '2023-11-23T02:01:06+00:00', 'page': 1, 'page_label': '2', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'total_pages': 24, 'creator': 'LaTeX with hyperref', 'author': '', 'source': 'gaiapaper.pdf', 'subject': ''}, page_content='Level 1\\nQuestion: What was the actual enrollment count of the clinical trial on H. pylori in acne vulgaris\\npatients from Jan-May 2018 as listed on the NIH website?\\nGround truth: 90\\nLevel 2\\nQuestion: If this whole pint is made up of ice cream, how many percent above\\nor below the US federal standards for butterfat content is it when using the\\nstandards as reported by Wikipedia in 2020? Answer as + or - a number rounded\\nto one decimal place.\\nGround truth: +4.6\\nLevel 3\\nQuestion: In NASA’s Astronomy Picture of the Day on 2006 January 21, two astronauts are visible,\\nwith one appearing much smaller than the other. As of August 2023, out of the astronauts in the\\nNASA Astronaut Group that the smaller astronaut was a member of, which one spent the least time\\nin space, and how many minutes did he spend in space, rounded to the nearest minute? Exclude any\\nastronauts who did not spend any time in space. Give the last name of the astronaut, separated from\\nthe number of minutes by a semicolon. Use commas as thousands separators in the number of minutes.\\nGround truth: White; 5876\\nFigure 1 Sample GAIA questions. Completing the tasks requires fundamental abilities such as reasoning, multi-\\nmodality handling, or tool use proficiency. Answers are unambiguous and by design unlikely to be found in plain text\\nin training data. Some questions come with additional evidence, such as images, reflecting real use cases and allowing\\nbetter control on the questions.\\nAlternatively to tasks that are harder for humans, AI systems could be asked to solve conceptually simple\\ntasks yet that require accurate execution of complex sequences of actions, with large combinatorial spaces.\\nThe output could only be obtained upon successful completion of the task and be easy to validate, analogous\\nto the Proof of Work algorithm (Jakobsson and Juels, 1999; Dwork and Naor, 1993), where a computer is\\nasked to solve a complex problem whose solution is easy to verify. Tasks for AI assistants, given their need for\\naccess to a diverse and uncertain world, meet this criterion while being inherently rooted in practical use\\ncases.\\nWe move in that direction by proposing GAIA, a benchmark for General AI Assistants featuring 466 carefully\\ncrafted questions and their answer, along with the associated design methodology. Our questions are easy\\nto create, challenging for AI systems—for LLMs, most require complex generations—, yet admit a unique,\\nfactual answer, allowing a simple and robust automatic evaluation.\\nGAIA attempts to avoid current pitfalls of LLMs evaluation by targeting:\\n- Real-world and challenging questions. For example, a LLM will typically need to browse the open and\\nchanging web, handle multi-modality, or reason over multiple steps to answer our questions. Conversely,\\nmany LLM benchmarks are quite specific and/or restricted to closed and synthetic environments.\\n- Easy interpretability through conceptually simple tasks—non experts annotators exhibit a near perfect\\nscore—, associated reasoning trace, and few but highly curated questions. This is in contrast with\\naggregated benchmarks that can lack efficiency and reliability (Perlitz et al., 2023).\\n- Non-gameability. Answering the questions requires successful completion of some number of steps, which\\ncannot easily be brute forced due to their diversity. The possibility to check the reasoning trace, the\\naccuracy required in the answers, their absence in plain text from the internet prevent a possible data\\ncontamination. In contrast, multiple choice answers ( e.g., MMLU) make contamination assessment more\\ndifficult since a wrong reasoning trace can more easily get to the correct choice.\\n- Simplicity of use. Crucially, the answers to our questions are factoid, concise and unambiguous. These\\n2')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Is Le Cunn one of the authors?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee4463",
   "metadata": {},
   "source": [
    "## Wrap into loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d51f7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def qa():\n",
    "    memory = ConversationBufferMemory(memory_key = \"chat_history\", return_message = True)\n",
    "    qa = ConversationalRetrievalChain.from_llm(llm=llm_main, \n",
    "                                               chain_type=\"stuff\", \n",
    "                                               retriever=docsearch.as_retriever(), \n",
    "                                               memory = memory, \n",
    "                                               get_chat_history=lambda h : h, \n",
    "                                               return_source_documents=False)\n",
    "    history = []\n",
    "    while True:\n",
    "        query = input(\"Question: \")\n",
    "        \n",
    "        if query.lower() in [\"quit\",\"exit\",\"bye\"]:\n",
    "            print(\"Answer: Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        result = qa({\"question\": query}, {\"chat_history\": history})\n",
    "        \n",
    "        history.append((query, result[\"answer\"]))\n",
    "        \n",
    "        print(\"Answer: \", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53b1ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2229421/1813949221.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key = \"chat_history\", return_message = True)\n",
      "/tmp/ipykernel_2229421/1813949221.py:22: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({\"question\": query}, {\"chat_history\": history})\n",
      "/mnt/data3/rrao/projects/agentic-reasoning-framework/arf-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  How's it going? Is there something I can help you with or would you like to chat?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data3/rrao/projects/agentic-reasoning-framework/arf-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  I'll do my best to provide quick answers by:\n",
      "\n",
      "1. Understanding your question clearly and accurately\n",
      "2. Referencing the provided context text, which is a detailed description of the ToolQA dataset and its creation process\n",
      "3. Providing concise and direct answers based on the information in the context text\n",
      "4. Not making assumptions or trying to fill in gaps in the information\n",
      "\n",
      "If I'm unsure or don't know the answer to your question, I'll say so instead of providing an incorrect or speculative response.\n",
      "\n",
      "Let's get started! What's your first question?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data3/rrao/projects/agentic-reasoning-framework/arf-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  I'll do my best to provide concise answers, typically within a few sentences (around 10-20 words). If your question requires more explanation or context, I may provide additional information in the form of short paragraphs. However, I won't exceed a few hundred words per answer unless absolutely necessary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data3/rrao/projects/agentic-reasoning-framework/arf-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  I don't know. The text doesn't specify a word limit for answering questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data3/rrao/projects/agentic-reasoning-framework/arf-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  According to the text, GAIA (General AI Assistant) is a benchmark for evaluating AI systems that propose general assistant questions. It consists of 466 human-annotated questions designed to test various capabilities such as reasoning, multi-modality understanding, coding, and tool use. The goal of GAIA is to evaluate how well an AI system can perform real-world tasks by answering these questions in a zero-shot setting.\n",
      "Answer: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa9a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
