{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f413c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOllama instance initialized with base_url: http://localhost:11434\n",
      "Tools bound: ['llm_summarize_10words', 'check_10words']\n",
      "\n",
      "--- Scenario 1: Invoking LLM with a tool-requiring query (Summarization) ---\n",
      "LLM's first response (AIMessage): content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-07-18T12:34:54.391660465Z', 'done': True, 'done_reason': 'stop', 'total_duration': 718137752, 'load_duration': 22615107, 'prompt_eval_count': 267, 'prompt_eval_duration': 3544864, 'eval_count': 64, 'eval_duration': 691356277, 'model_name': 'llama3.1:latest'} id='run--d41d7065-56f8-4bdb-934b-29b88f59ffc8-0' tool_calls=[{'name': 'llm_summarize_10words', 'args': {'txt': 'LangChain is a framework designed to simplify the creation of applications using large language models. It provides tools, chains, and agents to build complex LLM workflows. It helps developers build context-aware, reasoning applications.'}, 'id': '157eae0e-6f06-476d-bb5a-f29513271634', 'type': 'tool_call'}] usage_metadata={'input_tokens': 267, 'output_tokens': 64, 'total_tokens': 331}\n",
      "Content: ''\n",
      "Tool Calls: [{'name': 'llm_summarize_10words', 'args': {'txt': 'LangChain is a framework designed to simplify the creation of applications using large language models. It provides tools, chains, and agents to build complex LLM workflows. It helps developers build context-aware, reasoning applications.'}, 'id': '157eae0e-6f06-476d-bb5a-f29513271634', 'type': 'tool_call'}]\n",
      "\n",
      "--- Model decided to call a tool(s). Executing tool(s)... ---\n",
      "  Executing Tool: 'llm_summarize_10words' with arguments: {'txt': 'LangChain is a framework designed to simplify the creation of applications using large language models. It provides tools, chains, and agents to build complex LLM workflows. It helps developers build context-aware, reasoning applications.'}\n",
      "  Error executing tool 'llm_summarize_10words': BaseChatModel.invoke() missing 1 required positional argument: 'input'\n",
      "\n",
      "--- Re-invoking LLM with tool output for final response ---\n",
      "Final LLM Response (AIMessage): content='LangChain is a framework for simplifying large language model application creation.' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-07-18T12:34:54.604207268Z', 'done': True, 'done_reason': 'stop', 'total_duration': 209770279, 'load_duration': 22337046, 'prompt_eval_count': 201, 'prompt_eval_duration': 13228584, 'eval_count': 15, 'eval_duration': 172610241, 'model_name': 'llama3.1:latest'} id='run--6705fe02-7440-4aeb-9580-cd26c73e1dbc-0' usage_metadata={'input_tokens': 201, 'output_tokens': 15, 'total_tokens': 216}\n",
      "Final Content: 'LangChain is a framework for simplifying large language model application creation.'\n",
      "\n",
      "--- Scenario 2: Invoking LLM with a query for check_10words ---\n",
      "LLM's first response (AIMessage): content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-07-18T12:34:54.944267777Z', 'done': True, 'done_reason': 'stop', 'total_duration': 338124210, 'load_duration': 22445940, 'prompt_eval_count': 235, 'prompt_eval_duration': 13868653, 'eval_count': 28, 'eval_duration': 301243966, 'model_name': 'llama3.1:latest'} id='run--38363e98-1ba4-4408-b085-554a66474646-0' tool_calls=[{'name': 'check_10words', 'args': {'text': 'This is a test sentence with ten words exactly.'}, 'id': 'd0aee0e3-da41-4869-ba8a-1fbeb88d51d4', 'type': 'tool_call'}] usage_metadata={'input_tokens': 235, 'output_tokens': 28, 'total_tokens': 263}\n",
      "Content: ''\n",
      "Tool Calls: [{'name': 'check_10words', 'args': {'text': 'This is a test sentence with ten words exactly.'}, 'id': 'd0aee0e3-da41-4869-ba8a-1fbeb88d51d4', 'type': 'tool_call'}]\n",
      "\n",
      "--- Model decided to call a tool(s). Executing tool(s)... ---\n",
      "wtf\n",
      "{'name': 'check_10words', 'args': {'text': 'This is a test sentence with ten words exactly.'}, 'id': 'd0aee0e3-da41-4869-ba8a-1fbeb88d51d4', 'type': 'tool_call'}\n",
      "ftw\n",
      "  Executing Tool: 'check_10words' with arguments: {'text': 'This is a test sentence with ten words exactly.'}\n",
      "  Tool Output: 'False'\n",
      "\n",
      "--- Re-invoking LLM with tool output for final response ---\n",
      "Final LLM Response (AIMessage): content=\"The given text does not have exactly 10 words. It has 8 words: 'This', 'is', 'a', 'test', 'sentence', 'with', 'ten', 'words', 'exactly'.\" additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-07-18T12:34:55.422807936Z', 'done': True, 'done_reason': 'stop', 'total_duration': 475920559, 'load_duration': 22880755, 'prompt_eval_count': 117, 'prompt_eval_duration': 8298384, 'eval_count': 46, 'eval_duration': 443466340, 'model_name': 'llama3.1:latest'} id='run--e797c4f8-7634-4d44-8643-21bb68e5a8fd-0' usage_metadata={'input_tokens': 117, 'output_tokens': 46, 'total_tokens': 163}\n",
      "Final Content: 'The given text does not have exactly 10 words. It has 8 words: 'This', 'is', 'a', 'test', 'sentence', 'with', 'ten', 'words', 'exactly'.'\n",
      "\n",
      "--- Example of invoking LLM with a general query (no tool expected) ---\n",
      "LLM Response (AIMessage): content='' additional_kwargs={} response_metadata={'model': 'llama3.1:latest', 'created_at': '2025-07-18T12:34:55.863213189Z', 'done': True, 'done_reason': 'stop', 'total_duration': 438555947, 'load_duration': 23371625, 'prompt_eval_count': 225, 'prompt_eval_duration': 13604138, 'eval_count': 39, 'eval_duration': 401009544, 'model_name': 'llama3.1:latest'} id='run--501dffa8-bbe2-408d-b8d7-fb47cb7a19d3-0' tool_calls=[{'name': 'llm_summarize_10words', 'args': {'txt': 'An agent in LLMs refers to a software component that interacts with the model.'}, 'id': 'aaeb03bb-6945-449c-bcae-f7d6bc754a5e', 'type': 'tool_call'}] usage_metadata={'input_tokens': 225, 'output_tokens': 39, 'total_tokens': 264}\n",
      "Content: ''\n",
      "Tool Calls: [{'name': 'llm_summarize_10words', 'args': {'txt': 'An agent in LLMs refers to a software component that interacts with the model.'}, 'id': 'aaeb03bb-6945-449c-bcae-f7d6bc754a5e', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage, AIMessage\n",
    "import os\n",
    "from typing import Literal\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# --- Remote Ollama Server Configuration Notes (IMPORTANT!) ---\n",
    "# For your remote Ollama server to be accessible via http://localhost:11434 on your client machine,\n",
    "# you must have an SSH tunnel or similar port forwarding set up (e.g., `ssh -L 11434:localhost:11434 user@your_remote_server_ip`).\n",
    "#\n",
    "# On the remote server itself, before starting `ollama serve`, ensure these environment variables are set:\n",
    "# export OLLAMA_HOST=\"0.0.0.0\" # Allows Ollama to listen on all network interfaces\n",
    "# export OLLAMA_ORIGINS=\"http://<your_client_ip_or_domain>,http://localhost:11434\" # Allows CORS from your client\n",
    "# Replace <your_client_ip_or_domain> with the actual IP or domain of the machine running this client code.\n",
    "# If you are using an SSH tunnel, 'http://localhost:11434' might be sufficient for OLLAMA_ORIGINS on the server.\n",
    "\n",
    "# Set the base URL for ChatOllama to your local forwarded port\n",
    "# This assumes your SSH tunnel or port forwarding makes the remote Ollama accessible at this address.\n",
    "OLLAMA_CLIENT_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# --- Initialize ChatOllama instance ---\n",
    "# This is your main LLM instance that will be used for both direct queries and tool calls.\n",
    "llm_main = ChatOllama(\n",
    "    model=\"llama3.1:latest\", # Ensure this model is pulled on your remote Ollama server\n",
    "    temperature=0.0,\n",
    "    base_url=OLLAMA_CLIENT_BASE_URL, # Crucial: points to your accessible remote server (via tunnel) [1, 2]\n",
    "    api_key=\"ollama\" # Dummy value, as Ollama doesn't use API keys\n",
    ")\n",
    "\n",
    "# --- Define your tools ---\n",
    "# Note: The llm_summarize_10words tool needs access to an LLM itself.\n",
    "# We will pass the 'llm_main' instance to it.\n",
    "# For tools that call the LLM internally, it's important that the internal LLM instance\n",
    "# is correctly configured and available to the tool.\n",
    "_llm_for_internal_tool_use = llm_main\n",
    "\n",
    "@tool\n",
    "def llm_summarize_10words(txt: str) -> str:\n",
    "    \"\"\"Summarize a text down to 10 words.\"\"\"\n",
    "    # This tool uses the main LLM instance to perform its summarization task.\n",
    "    if _llm_for_internal_tool_use is None:\n",
    "        raise RuntimeError(\"LLM for internal tool use not initialized.\")\n",
    "    # CORRECTED: Pass the HumanMessage with the text to summarize to the LLM\n",
    "    res = _llm_for_internal_tool_use.invoke()\n",
    "    return res.content\n",
    "\n",
    "@tool\n",
    "def check_10words(text: str) -> bool:\n",
    "    \"\"\"Check if a text contains exactly 10 words.\"\"\"\n",
    "    return len(text.split()) == 10\n",
    "\n",
    "# --- Bind tools to the LLM instance ---\n",
    "tools = [llm_summarize_10words, check_10words]\n",
    "llm_with_tools = llm_main.bind_tools(tools) \n",
    "\n",
    "print(f\"ChatOllama instance initialized with base_url: {llm_main.base_url}\")\n",
    "print(f\"Tools bound: {[t.name for t in tools]}\")\n",
    "\n",
    "# --- Agentic Loop Demonstration ---\n",
    "\n",
    "# Scenario 1: Query that should trigger a tool call (summarization)\n",
    "print(\"\\n--- Scenario 1: Invoking LLM with a tool-requiring query (Summarization) ---\")\n",
    "text_to_summarize = \"LangChain is a framework designed to simplify the creation of applications using large language models. It provides tools, chains, and agents to build complex LLM workflows. It helps developers build context-aware, reasoning applications.\"\n",
    "query_summarize = f\"Please summarize the following text in exactly 10 words: '{text_to_summarize}'\"\n",
    "\n",
    "# Step 1: Invoke the LLM with the user's query\n",
    "response_message_1 = llm_with_tools.invoke([HumanMessage(query_summarize)]) \n",
    "print(f\"LLM's first response (AIMessage): {response_message_1}\")\n",
    "print(f\"Content: '{response_message_1.content}'\")\n",
    "print(f\"Tool Calls: {response_message_1.tool_calls}\")\n",
    "\n",
    "# Step 2: Check for tool calls and execute them\n",
    "if response_message_1.tool_calls:\n",
    "    print(\"\\n--- Model decided to call a tool(s). Executing tool(s)... ---\")\n",
    "    messages_for_next_turn = [HumanMessage(query_summarize), response_message_1]\n",
    "\n",
    "    for tool_call in response_message_1.tool_calls: # Corrected indentation [3, 5]\n",
    "        tool_name = tool_call['name']\n",
    "        tool_args = tool_call['args']\n",
    "        call_id = tool_call['id'] # Important for ToolMessage [3]\n",
    "\n",
    "        print(f\"  Executing Tool: '{tool_name}' with arguments: {tool_args}\")\n",
    "\n",
    "        # Map tool names to actual Python functions\n",
    "        available_tools_map = {\n",
    "            \"llm_summarize_10words\": llm_summarize_10words,\n",
    "            \"check_10words\": check_10words\n",
    "        }\n",
    "\n",
    "        if tool_name in available_tools_map:\n",
    "            selected_tool_func = available_tools_map[tool_name]\n",
    "            try:\n",
    "                # Execute the tool. For LangChain tools,.invoke() is the method.\n",
    "                # Ensure arguments are passed correctly (e.g., as kwargs if tool expects them).\n",
    "                tool_output = selected_tool_func.invoke(tool_args)\n",
    "                print(f\"  Tool Output: '{tool_output}'\")\n",
    "\n",
    "                # Append the ToolMessage to the conversation history [6, 7]\n",
    "                messages_for_next_turn.append(\n",
    "                    ToolMessage(content=str(tool_output), tool_call_id=call_id)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"  Error executing tool '{tool_name}': {e}\")\n",
    "                messages_for_next_turn.append(\n",
    "                    ToolMessage(content=f\"Error: {e}\", tool_call_id=call_id)\n",
    "                )\n",
    "        else:\n",
    "            print(f\"  Error: Tool '{tool_name}' not found in available functions map.\")\n",
    "            messages_for_next_turn.append(\n",
    "                ToolMessage(content=f\"Error: Tool '{tool_name}' not found.\", tool_call_id=call_id)\n",
    "            )\n",
    "\n",
    "    # Step 3: Re-invoke the LLM with the tool output for the final response\n",
    "    print(\"\\n--- Re-invoking LLM with tool output for final response ---\")\n",
    "    final_response_message = llm_with_tools.invoke(messages_for_next_turn)\n",
    "    print(f\"Final LLM Response (AIMessage): {final_response_message}\")\n",
    "    print(f\"Final Content: '{final_response_message.content}'\")\n",
    "else:\n",
    "    print(\"\\nModel did not decide to call any tools for the first query. Content:\")\n",
    "    print(f\"'{response_message_1.content}'\")\n",
    "\n",
    "# Scenario 2: Query that might trigger a different tool or no tool\n",
    "print(\"\\n--- Scenario 2: Invoking LLM with a query for check_10words ---\")\n",
    "text_to_check = \"This is a test sentence with ten words exactly.\"\n",
    "query_check_words = f\"Check if the following text has exactly 10 words: '{text_to_check}'\"\n",
    "\n",
    "response_message_2 = llm_with_tools.invoke([HumanMessage(query_check_words)])\n",
    "print(f\"LLM's first response (AIMessage): {response_message_2}\")\n",
    "print(f\"Content: '{response_message_2.content}'\")\n",
    "print(f\"Tool Calls: {response_message_2.tool_calls}\")\n",
    "\n",
    "if response_message_2.tool_calls:\n",
    "    print(\"\\n--- Model decided to call a tool(s). Executing tool(s)... ---\")\n",
    "    messages_for_next_turn_2 = [HumanMessage(query_check_words), response_message_2]\n",
    "\n",
    "    for tool_call in response_message_2.tool_calls: # Corrected indentation\n",
    "        print(\"wtf\")\n",
    "        print(tool_call)\n",
    "        print(\"ftw\")\n",
    "        tool_name = tool_call['name']\n",
    "        tool_args = tool_call['args']\n",
    "        call_id = tool_call['id']\n",
    "\n",
    "        print(f\"  Executing Tool: '{tool_name}' with arguments: {tool_args}\")\n",
    "        available_tools_map = {\n",
    "            \"llm_summarize_10words\": llm_summarize_10words,\n",
    "            \"check_10words\": check_10words\n",
    "        }\n",
    "\n",
    "        if tool_name in available_tools_map:\n",
    "            selected_tool_func = available_tools_map[tool_name]\n",
    "            try:\n",
    "                tool_output = selected_tool_func.invoke(tool_args)\n",
    "                print(f\"  Tool Output: '{tool_output}'\")\n",
    "                messages_for_next_turn_2.append(\n",
    "                    ToolMessage(content=str(tool_output), tool_call_id=call_id)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"  Error executing tool '{tool_name}': {e}\")\n",
    "                messages_for_next_turn_2.append(\n",
    "                    ToolMessage(content=f\"Error: {e}\", tool_call_id=call_id)\n",
    "                )\n",
    "        else:\n",
    "            print(f\"  Error: Tool '{tool_name}' not found in available functions map.\")\n",
    "            messages_for_next_turn_2.append(\n",
    "                ToolMessage(content=f\"Error: Tool '{tool_name}' not found.\", tool_call_id=call_id)\n",
    "            )\n",
    "\n",
    "    print(\"\\n--- Re-invoking LLM with tool output for final response ---\")\n",
    "    final_response_message_2 = llm_with_tools.invoke(messages_for_next_turn_2)\n",
    "    print(f\"Final LLM Response (AIMessage): {final_response_message_2}\")\n",
    "    print(f\"Final Content: '{final_response_message_2.content}'\")\n",
    "else:\n",
    "    print(\"\\nModel did not decide to call any tools for the second query. Content:\")\n",
    "    print(f\"'{response_message_2.content}'\")\n",
    "\n",
    "print(\"\\n--- Example of invoking LLM with a general query (no tool expected) ---\")\n",
    "query_general = \"What is an agent in the context of LLMs?\"\n",
    "response_general = llm_with_tools.invoke([HumanMessage(query_general)])\n",
    "print(f\"LLM Response (AIMessage): {response_general}\")\n",
    "print(f\"Content: '{response_general.content}'\")\n",
    "print(f\"Tool Calls: {response_general.tool_calls}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
